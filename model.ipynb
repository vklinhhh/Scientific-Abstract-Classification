{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/MasterClass/PPNCKH/ppnckh_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/mac/Desktop/MasterClass/PPNCKH/Data/train.csv', names=[\"document\", \"class\"], header=0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In this doctoral thesis, we apply premises o...</td>\n",
       "      <td>CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We describe an LSTM-based model which we cal...</td>\n",
       "      <td>CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We consider the cross-domain sentiment class...</td>\n",
       "      <td>CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In this paper we present the approach of int...</td>\n",
       "      <td>CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Generative Adversarial Networks (GANs) have ...</td>\n",
       "      <td>CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>Inter-package conflicts require the presence...</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>Background: To adequately attend to non-func...</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>Background: Bots help automate many of the t...</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>One major problem in maintaining a software ...</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>Self-adaptivity allows software systems to a...</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              document class\n",
       "0      In this doctoral thesis, we apply premises o...    CL\n",
       "1      We describe an LSTM-based model which we cal...    CL\n",
       "2      We consider the cross-domain sentiment class...    CL\n",
       "3      In this paper we present the approach of int...    CL\n",
       "4      Generative Adversarial Networks (GANs) have ...    CL\n",
       "..                                                 ...   ...\n",
       "695    Inter-package conflicts require the presence...    SE\n",
       "696    Background: To adequately attend to non-func...    SE\n",
       "697    Background: Bots help automate many of the t...    SE\n",
       "698    One major problem in maintaining a software ...    SE\n",
       "699    Self-adaptivity allows software systems to a...    SE\n",
       "\n",
       "[700 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_cl = train_data[train_data[\"class\"]=='CL'].iloc[:100].reset_index()\n",
    "train_data_cr = train_data[train_data[\"class\"]=='CR'].iloc[:100].reset_index()\n",
    "train_data_dc = train_data[train_data[\"class\"]=='DC'].iloc[:100].reset_index()\n",
    "train_data_ds = train_data[train_data[\"class\"]=='DS'].iloc[:100].reset_index()\n",
    "train_data_lo = train_data[train_data[\"class\"]=='LO'].iloc[:100].reset_index()\n",
    "train_data_ni = train_data[train_data[\"class\"]=='NI'].iloc[:100].reset_index()\n",
    "train_data_se = train_data[train_data[\"class\"]=='SE'].iloc[:100].reset_index()\n",
    "train_data = pd.concat([train_data_cl,train_data_cr,train_data_dc,train_data_ds,train_data_lo,train_data_ni,train_data_se],ignore_index=True).drop([\"index\"], axis=\"columns\")\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Humor is a defining characteristic of human ...</td>\n",
       "      <td>CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Auto-encoders compress input data into a lat...</td>\n",
       "      <td>CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Many common character-level, string-to-strin...</td>\n",
       "      <td>CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frequency is one of the major factors for tr...</td>\n",
       "      <td>CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Centrality of emotion for the stories told b...</td>\n",
       "      <td>CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The context-awareness of things that belong ...</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Automatic test data generation (ATG) is a ma...</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Non-functional requirements (NFRs) are deter...</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>In the context of robustness testing, the bo...</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Delta modeling is a modular, yet flexible ap...</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             document class\n",
       "0     Humor is a defining characteristic of human ...    CL\n",
       "1     Auto-encoders compress input data into a lat...    CL\n",
       "2     Many common character-level, string-to-strin...    CL\n",
       "3     Frequency is one of the major factors for tr...    CL\n",
       "4     Centrality of emotion for the stories told b...    CL\n",
       "..                                                ...   ...\n",
       "15    The context-awareness of things that belong ...    SE\n",
       "16    Automatic test data generation (ATG) is a ma...    SE\n",
       "17    Non-functional requirements (NFRs) are deter...    SE\n",
       "18    In the context of robustness testing, the bo...    SE\n",
       "19    Delta modeling is a modular, yet flexible ap...    SE\n",
       "\n",
       "[140 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_cl = test_data[test_data[\"class\"]=='CL'].iloc[:20].reset_index()\n",
    "test_data_cr = test_data[test_data[\"class\"]=='CR'].iloc[:20].reset_index()\n",
    "test_data_dc = test_data[test_data[\"class\"]=='DC'].iloc[:20].reset_index()\n",
    "test_data_ds = test_data[test_data[\"class\"]=='DS'].iloc[:20].reset_index()\n",
    "test_data_lo = test_data[test_data[\"class\"]=='LO'].iloc[:20].reset_index()\n",
    "test_data_ni = test_data[test_data[\"class\"]=='NI'].iloc[:20].reset_index()\n",
    "test_data_se = test_data[test_data[\"class\"]=='SE'].iloc[:20].reset_index()\n",
    "test_data = pd.concat([test_data_cl,test_data_cr,test_data_dc,test_data_ds,test_data_lo,test_data_ni,test_data_se], ignore_index=False).drop([\"index\"], axis=\"columns\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(train_data['document'])\n",
    "tfidf_test = tfidf_vectorizer.transform(test_data['document'])\n",
    "\n",
    "# Train a Logistic Regression model on TF-IDF\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(tfidf_train, train_data['class'])\n",
    "lr_predictions = lr_model.predict(tfidf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and tokenize the RoBERTa model\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=7)\n",
    "device = torch.device(\"cpu\")\n",
    "roberta_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for RoBERTa\n",
    "def tokenize_text(texts, tokenizer, max_length):\n",
    "    inputs = tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    return inputs\n",
    "\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "\n",
    "# Preprocess training data for RoBERTa\n",
    "roberta_train_inputs = tokenize_text(train_data['document'].tolist(), roberta_tokenizer, max_length)\n",
    "with torch.no_grad():\n",
    "    roberta_train_outputs = roberta_model(**roberta_train_inputs.to(device), output_hidden_states= True)[0].cpu().detach().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=10, random_state=42)  # Adjust the number of components as needed\n",
    "lda_topics_train = lda_model.fit_transform(tfidf_train)\n",
    "\n",
    "# Combine RoBERTa embeddings with LDA topics\n",
    "combined_features_train = np.concatenate((roberta_train_outputs, lda_topics_train), axis=1)\n",
    "# Train a Logistic Regression model on combined features\n",
    "lr_combined_model = LogisticRegression(max_iter=1000)\n",
    "lr_combined_model.fit(combined_features_train, train_data['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data for RoBERTa and LDA\n",
    "roberta_test_inputs = tokenize_text(test_data['document'].tolist(), roberta_tokenizer, max_length)\n",
    "with torch.no_grad():\n",
    "    roberta_test_outputs = roberta_model(**roberta_test_inputs.to(device), output_hidden_states= True)[0].cpu().detach().numpy()\n",
    "\n",
    "lda_topics_test = lda_model.transform(tfidf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine test features\n",
    "combined_features_test = np.concatenate((roberta_test_outputs, lda_topics_test), axis=1)\n",
    "\n",
    "# Make predictions using the ensemble model\n",
    "lr_predictions = lr_model.predict(tfidf_test)\n",
    "lr_combined_predictions = lr_combined_model.predict(combined_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy: 0.65\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CL       0.51      0.95      0.67        20\n",
      "          CR       0.60      0.60      0.60        20\n",
      "          DC       0.59      0.65      0.62        20\n",
      "          DS       0.82      0.70      0.76        20\n",
      "          LO       0.81      0.65      0.72        20\n",
      "          NI       0.63      0.60      0.62        20\n",
      "          SE       0.89      0.40      0.55        20\n",
      "\n",
      "    accuracy                           0.65       140\n",
      "   macro avg       0.69      0.65      0.65       140\n",
      "weighted avg       0.69      0.65      0.65       140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def majority_vote(predictions_list, label_encoder):\n",
    "    final_predictions = []\n",
    "    for i in range(len(predictions_list[0])):\n",
    "        counts = np.bincount([label_encoder.transform([preds[i]])[0] for preds in predictions_list])\n",
    "        final_predictions.append(np.argmax(counts))\n",
    "    return final_predictions\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create a LabelEncoder and fit it on the train labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_data['class'])\n",
    "\n",
    "# Use the majority_vote function\n",
    "final_predictions = majority_vote([lr_predictions, lr_combined_predictions], label_encoder)\n",
    "\n",
    "# Transform test labels using the label_encoder\n",
    "test_labels_encoded = label_encoder.transform(test_data['class'])\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy = accuracy_score(test_labels_encoded, final_predictions)\n",
    "print(f\"Ensemble Model Accuracy: {accuracy}\")\n",
    "\n",
    "# Print the classification report\n",
    "class_names = label_encoder.classes_\n",
    "print(classification_report(test_labels_encoded, final_predictions, target_names=class_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    \"CL\":\"Computation and Language\",\n",
    "    \"CR\": \"Cryptography and Security\",\n",
    "    \"DC\": \"Distributed and Cluster Computing\",\n",
    "    \"DS\": \"Data Structures and Algorithms\",\n",
    "    \"LO\": \"Logic in Computer Science\",\n",
    "    \"NI\": \"Networking and Internet Architecture\",\n",
    "    \"SE\": \"Software Engineer\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    text_input = input()\n",
    "\n",
    "    roberta_test_inputs = tokenize_text([text_input], roberta_tokenizer, max_length)\n",
    "    with torch.no_grad():\n",
    "        roberta_test_outputs = roberta_model(**roberta_test_inputs.to(device), output_hidden_states=True)[0].cpu().detach().numpy()\n",
    "\n",
    "    tfidf_input = tfidf_vectorizer.transform([text_input])\n",
    "    lr_pred = lr_model.predict(tfidf_input)\n",
    "\n",
    "    # Get LDA topics for the input\n",
    "    lda_topic = lda_model.transform(tfidf_input)\n",
    "\n",
    "    # Combine test features for the ensemble model\n",
    "    combined_features_test = np.concatenate((roberta_test_outputs, lda_topic), axis=1)\n",
    "\n",
    "    # Make predictions using the ensemble model\n",
    "    lr_combined_pred = lr_combined_model.predict(combined_features_test)\n",
    "\n",
    "    # Use the majority_vote function\n",
    "    final_prediction = majority_vote([lr_pred, lr_combined_pred], label_encoder)\n",
    "\n",
    "    # Decode the predicted class label\n",
    "    predicted_label = label_encoder.inverse_transform([final_prediction])[0]\n",
    "    # Map predicted class abbreviation to full context\n",
    "    full_label = class_mapping.get(predicted_label, \"Unknown\")\n",
    "\n",
    "    print(\"Predicted Class:\", full_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: Computation and Language\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/MasterClass/PPNCKH/ppnckh_venv/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:153: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppnckh_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
